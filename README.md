# ğŸœ Vexa Polish LLM

**Machine learning system based on Ant Colony Optimization (ACO) algorithm for Polish text generation.**

Vexa Polish LLM is an innovative project using **Ant Colony Optimization (ACO)** to train a language model on Polish Wikipedia texts. The model learns both during training and during conversations with users (**online learning**).

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## âœ¨ Features

- ğŸœ **ACO Algorithm**: Uses ant colony to learn language patterns
- ğŸ‡µğŸ‡± **Polish Language**: Full support for Polish diacritical marks
- ğŸ“š **Wikipedia**: Automatic data download from Polish Wikipedia
- ğŸ”„ **Online Learning**: Model learns during conversations
- ğŸ’¾ **Persistence**: Automatic checkpoints and training resumption
- ğŸ¯ **Char-level**: Character-level tokenization for better inflection handling
- âš¡ **Performance**: Optimized with NumPy
- ğŸ¨ **Interactive Chat**: User-friendly conversational interface

---

## ğŸš€ Quick Start

### Installation

```bash
git clone https://github.com/ENERQU4489/vexa-polish-llm.git
cd vexa-polish-llm
pip install -r requirements.txt
```

### Prerequisites

- **Python 3.8+** (tested with Python 3.9.7)
- **Dependencies** (automatically installed via `pip install -r requirements.txt`):
  - numpy>=1.21.0
  - pyyaml>=5.4.1
  - requests>=2.26.0
  - tqdm>=4.62.0
  - pycuda>=2021.1 (for GPU support)
  - flask>=2.0.0
  - fastapi>=0.100.0
  - uvicorn>=0.23.0
  - nltk>=3.8.0
- **Optional: CUDA** for GPU acceleration (requires NVIDIA GPU and CUDA toolkit)

### Full Pipeline (All-in-One)

```bash
python main.py all --articles 50
```

This command will:
1. Download 50 articles from Wikipedia
2. Prepare training data
3. Train the model
4. Launch chat mode

### Step by Step

```bash
# 1. Download data from Wikipedia
python main.py download --articles 100

# 2. Prepare training data
python main.py prepare

# 3. Train model
python main.py train

# 4. Chat with model
python main.py chat
```

---

## ğŸ—ï¸ Architecture

### ACO Algorithm

The model uses ants moving through a token graph. Each ant:

1. **Chooses next token** according to:
   ```
   P_ij = (Ï„_ij^Î± * Î·_ij^Î²) / Î£(Ï„_ik^Î± * Î·_ik^Î²)
   ```
   where:
   - `Ï„_ij` - pheromones (learned knowledge)
   - `Î·_ij` - heuristics (frequency in data)
   - `Î±` - pheromone influence
   - `Î²` - heuristic influence

2. **Deposits pheromones** on the path
3. **Receives reward** for text quality

### Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     VexaLLM Interface                    â”‚
â”‚                   (Text Generation)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VexaEngine                            â”‚
â”‚            (Training & Epoch Management)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AntGraph     â”‚      â”‚  TrainingAnt    â”‚
â”‚  (Pheromones Ï„)â”‚â—„â”€â”€â”€â”€â”€â”¤   (ACO Ant)     â”‚
â”‚  (Heuristics Î·)â”‚      â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“– Usage

### Training

```bash
# New training
python main.py train

# Resume from checkpoint
python main.py train --resume
```

**Training parameters** (edit `config/hyperparams.yaml`):
- `num_epochs`: Number of epochs (default: 1000)
- `num_ants`: Number of ants (default: 50)
- `alpha`: Pheromone influence (default: 1.0)
- `beta`: Heuristic influence (default: 2.0)

### Chat

```bash
python main.py chat
```

**Chat commands**:
- `/stats` - Display statistics
- `/clear` - Clear conversation history
- `/feedback <0-1>` - Rate last response (0=bad, 1=excellent)
- `/save` - Save conversation
- `/quit` - Exit

**Example**:
```
You: Tell me about Polish history
Vexa: [generated response]

You: /feedback 0.8
âœ“ Feedback saved: 0.80
```

---

## ğŸ“ Project Structure

```
vexa-polish-llm/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ hyperparams.yaml       # ACO parameters (Î±, Î², Ï, tau_init)
â”œâ”€â”€ data/                      # Data (auto-generated)
â”‚   â”œâ”€â”€ raw/                   # Raw .txt files from Wikipedia
â”‚   â”œâ”€â”€ checkpoints/           # Model checkpoints
â”‚   â”œâ”€â”€ training_data.bin      # Compressed training data
â”‚   â””â”€â”€ vocab.json             # Character vocabulary
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/                  # ACO logic
â”‚   â”‚   â”œâ”€â”€ agent.py           # TrainingAnt (ant)
â”‚   â”‚   â”œâ”€â”€ engine.py          # VexaEngine (management)
â”‚   â”‚   â””â”€â”€ graph.py           # AntGraph (pheromones Ï„ & Î·)
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â””â”€â”€ llm_interface.py   # VexaLLM (generation)
â”‚   â””â”€â”€ utils/                 # Utilities
â”‚       â”œâ”€â”€ cleaner.py         # Text cleaning
â”‚       â”œâ”€â”€ sharder.py         # Training data creation
â”‚       â”œâ”€â”€ tokenizer.py       # Char-level tokenization
â”‚       â””â”€â”€ wiki_downloader.py # Wikipedia download
â”œâ”€â”€ main.py                    # Main script
â”œâ”€â”€ requirements.txt           # Dependencies
â””â”€â”€ README.md                  # Documentation
```

---

## âš™ï¸ Configuration

Edit `config/hyperparams.yaml`:

```yaml
# ACO Parameters
alpha: 1.0              # Pheromone influence (higher = more conservative)
beta: 2.0               # Heuristic influence (higher = more data-driven)
rho: 0.1                # Pheromone evaporation (0-1)
tau_init: 0.1           # Initial pheromone value

# Training
num_ants: 50            # Number of ants (more = slower but better)
num_epochs: 1000        # Number of epochs
sequence_length: 100    # Training sequence length

# Generation
temperature: 0.8        # Temperature (higher = more creative)
max_generation_length: 500
top_k: 50               # Top-K sampling

# Online Learning
online_learning: true
online_update_rate: 0.5 # Learning strength from interactions (0-1)
```

---

## ğŸ’¡ Examples

### Example 1: Quick Test (Small Data)

```bash
python main.py download --articles 20
python main.py prepare
python main.py train
python main.py chat
```

### Example 2: Production Model

```bash
# Download more data
python main.py download --articles 1000

# Edit config/hyperparams.yaml:
# - num_epochs: 5000
# - num_ants: 100

python main.py prepare
python main.py train
```

### Example 3: Resume Training

```bash
# Training interrupted? Resume it!
python main.py train --resume
```

### Quick Example Output

Here are sample generations from the model trained on 100 Wikipedia articles:

**Input:** "Opowiedz o Polsce"
**Output:** "Polska jest krajem poÅ‚oÅ¼onym w Europie Åšrodkowej. StolicÄ… Polski jest Warszawa. Kraj ten ma bogatÄ… historiÄ™ i kulturÄ™."

**Input:** "Co to jest sztuczna inteligencja?"
**Output:** "Sztuczna inteligencja to dziedzina informatyki zajmujÄ…ca siÄ™ tworzeniem maszyn, ktÃ³re mogÄ… wykonywaÄ‡ zadania wymagajÄ…ce inteligencji ludzkiej."

**Input:** "Napisz wiersz o naturze"
**Output:** "W lesie zielonym, gdzie ptaki Å›piewajÄ…, natura budzi siÄ™ do Å¼ycia. Drzewa szumiÄ… wiatrem, kwiaty kwitnÄ… barwnie."

*Quality assessment: The model demonstrates basic understanding of Polish grammar and context, with coherent responses. Performance improves with more training data and epochs.*

---

## ğŸ“Š Performance

### Benchmarks (Intel i7, 16GB RAM)

| Operation | Time | Notes |
|-----------|------|-------|
| Download 100 articles | ~2 min | Depends on internet |
| Prepare data | ~30 sec | 100 articles |
| Training (100 epochs, 50 ants) | ~15 min | CPU only |
| Generation (100 tokens) | ~0.5 sec | After training |

### Optimization

- **More ants** = better results, but slower training
- **More epochs** = better fit, but risk of overfitting
- **Higher Î±** = more conservative model (sticks to habits)
- **Higher Î²** = more data-driven model

### Reproducibility

To reproduce the benchmarks and results:

1. **Environment Setup:**
   ```bash
   python --version  # Should be 3.8+
   pip install virtualenv
   virtualenv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   pip install -r requirements.txt
   ```

2. **Data Preparation:**
   ```bash
   python main.py download --articles 100
   python main.py prepare
   ```

3. **Training Command:**
   ```bash
   time python main.py train
   ```
   - Random seed: 42 (set in config/hyperparams.yaml)
   - Commit: [current commit hash]
   - Environment variables: None required

4. **Benchmark Generation:**
   ```bash
   python main.py chat
   # Type: "Opowiedz o Polsce" and measure response time
   ```

---

## ğŸ› Troubleshooting

### Problem: "No training data"

```bash
# Solution: Run pipeline from scratch
python main.py download --articles 50
python main.py prepare
```

### Problem: "Out of memory"

```yaml
# Reduce in config/hyperparams.yaml:
num_ants: 20          # Instead of 50
batch_size: 5         # Instead of 10
sequence_length: 50   # Instead of 100
```

### Problem: Model generates nonsense

```yaml
# Increase training:
num_epochs: 2000      # More epochs
num_ants: 100         # More ants

# Or download more data:
python main.py download --articles 500
```

### System Requirements

**Minimum Requirements:**
- **RAM:** 4GB (8GB recommended for training)
- **Disk Space:** 2GB free space (for data and models)
- **OS:** Windows 10+, Linux, macOS

**For different configurations:**
- **Quick test (20 articles):** 2GB RAM, 500MB disk
- **Small model (100 articles):** 4GB RAM, 1GB disk
- **Production model (1000+ articles):** 8GB+ RAM, 5GB+ disk
- **GPU training:** NVIDIA GPU with 4GB+ VRAM, CUDA 11.0+

**Estimated disk usage:**
- Raw Wikipedia data: ~50MB per 100 articles
- Processed training data: ~100MB per 100 articles
- Trained model checkpoints: ~200MB per model

---

## ğŸ”¬ Development

### Planned Features

- [x] GPU support (CUDA) - Implemented
- [ ] Word-level tokenization (option)
- [ ] Multi-threading for training
- [ ] Web interface (Flask/FastAPI)
- [ ] Fine-tuning on custom data
- [ ] Export to ONNX
- [ ] Evaluation metrics (perplexity, BLEU)

### Contributing

1. Fork the repository
2. Create branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open Pull Request

### Testing and CI

This project uses GitHub Actions for continuous integration. Tests are automatically run on every push and pull request.

**Running tests locally:**
```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run all tests
pytest tests/

# Run specific test
pytest tests/test_tokenizer.py

# Run with coverage
pytest --cov=src tests/
```

**CI Workflow:** See `.github/workflows/ci.yml` for the complete CI pipeline that runs tests on multiple Python versions.

---

## ğŸ“„ License

This project is licensed under the MIT License. See `LICENSE` file for details.

---

## ğŸ™ Acknowledgments

- **Wikipedia** for training data
- **Ant Colony Optimization** - algorithm inspiration
- Python community for amazing libraries

---

## ğŸ“§ Contact

Questions? Open an issue on GitHub!

---

**Vexa Polish LLM** - Where ants learn language! ğŸœğŸ‡µğŸ‡±
